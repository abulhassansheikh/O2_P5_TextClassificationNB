{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Series/Part Type classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob as gl\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStringColumn(df, Orgcol):\n",
    "    col = Orgcol+\"_new\"\n",
    "    df[col] = df[Orgcol]\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = df[col].str.replace(r'[^\\w]', ' ')\n",
    "    df[col] = df[col].str.replace(r'[^a-zA-Z\\s]+', ' ')\n",
    "\n",
    "    df[col] = df[col].str.replace(r'\\s\\w\\s', ' ')\n",
    "    df[col] = df[col].str.replace(r'\\s\\w\\w\\s' , ' ')\n",
    "    df[col] = df[col].str.replace(r'\\s\\w\\s', ' ')\n",
    "    df[col] = df[col].str.replace(r'\\s\\w\\w\\s', ' ')\n",
    "    df[col] = df[col].str.replace(r'^\\w\\s', ' ')\n",
    "    df[col] = df[col].str.replace(r'^\\w\\w\\s', ' ')\n",
    "    df[col] = df[col].str.replace(r'\\s\\w$' , ' ')\n",
    "    df[col] = df[col].str.replace(r'\\s\\w\\w$', ' ')\n",
    "    df[col] = df[col].str.replace(r'^\\w$', ' ')\n",
    "    df[col] = df[col].str.replace(r'^\\w\\w$', ' ')\n",
    "    df[col] = df[col].str.replace(r'^\\w$', ' ')\n",
    "    df[col] = df[col].str.replace(r'^\\w\\w$', ' ')\n",
    "    \n",
    "    df[col] = df[col].str.replace(r'   ', ' ')\n",
    "    df[col] = df[col].str.replace(r'  ', ' ')\n",
    "    df[col] = df[col].str.replace(r'   ', ' ')\n",
    "    df[col] = df[col].str.replace(r'  ', ' ')\n",
    "    df[col] = df[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Training/Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pool together data from Jobbers\n",
    "CompiledJobber = pd.DataFrame(columns = [\"sku\",\"brand\",\"JobberString\"])\n",
    "\n",
    "AllBrandFolderPath = \"//192.168.2.32/Group/Data Team/Brand_Update_Location/1_Input_Folder\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "AllBrandFolders = pd.DataFrame({\"File\":AllBrandFolders})\n",
    "AllBrandFolders = AllBrandFolders[~(AllBrandFolders['File'].str.contains(\"NS--\"))]\n",
    "AllBrandFolders = list(AllBrandFolders[\"File\"])\n",
    "\n",
    "for b in range(len(AllBrandFolders)):\n",
    "    print((len(AllBrandFolders)-b))\n",
    "    brand = AllBrandFolders[b].replace('.csv', '')\n",
    "\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+AllBrandFolders[b])\n",
    "    BrandJobber = pd.read_csv(BrandPath, encoding='mac_roman')\n",
    "\n",
    "    BrandJobber = BrandJobber.drop(columns=[\"usa_jobber_price\", \"ca_jobber_price\", \n",
    "                                            \"Jobber_UPC\", \"Jobber_Weight\", \"Jobber_Length\", \n",
    "                                            \"Jobber_Width\", \"Jobber_Height\"])\n",
    "\n",
    "    Coldf = pd.DataFrame({\"type\": BrandJobber.dtypes}).reset_index()[1:]\n",
    "    Coldf = Coldf[Coldf[\"type\"] == \"object\"]\n",
    "    Cols = list(Coldf[\"index\"])\n",
    "    newCol = []\n",
    "\n",
    "\n",
    "    for c in range(len(Cols)):\n",
    "        String = (BrandJobber[Cols[c]].str.replace('[^A-Za-z0-9]+', ' ')).sort_values(ascending = False).reset_index().iloc[1,1]\n",
    "        String = String.lower()\n",
    "        colLen = len(BrandJobber[Cols[c]].unique())\n",
    "        if ((String.isdigit() == False) & (colLen >1)):\n",
    "            if ((\"www\" in String) == False | (\"http\" in String) == False | \n",
    "                (\"jpg\" in String) == False | (\"cancer\" in String) == False |\n",
    "                (\"reproductive\" in String) == False):\n",
    "                newCol.append(Cols[c])\n",
    "\n",
    "\n",
    "    if (len(newCol)) == 0:\n",
    "        BrandJobberClean = pd.DataFrame({\"sku\": BrandJobber[\"sku\"], \n",
    "                      \"brand\": brand,\n",
    "                      \"JobberString\": \"\"\n",
    "                     })\n",
    "\n",
    "        CompiledJobber = CompiledJobber.append(BrandJobberClean, ignore_index=True)\n",
    "\n",
    "    elif (len(newCol)) == 1:\n",
    "        BrandJobberClean = pd.DataFrame({\"sku\": BrandJobber[\"sku\"], \n",
    "                      \"brand\": brand,\n",
    "                      \"JobberString\":\n",
    "                      BrandJobber[(BrandJobber.columns)[1]]\n",
    "                     })\n",
    "\n",
    "        CompiledJobber = CompiledJobber.append(BrandJobberClean, ignore_index=True)\n",
    "\n",
    "    elif (len(newCol)) > 1:\n",
    "\n",
    "        JobberString = \"\"\n",
    "        for t in range(len(newCol)):\n",
    "            JobberString = JobberString + \" \" + BrandJobber[newCol[t]].fillna('').map(str)\n",
    "\n",
    "        BrandJobberClean = pd.DataFrame({\"sku\": BrandJobber[\"sku\"], \n",
    "                      \"brand\": brand,\n",
    "                      \"JobberString\": JobberString\n",
    "                     })\n",
    "\n",
    "        CompiledJobber = CompiledJobber.append(BrandJobberClean, ignore_index=True)\n",
    "\n",
    "#To change\n",
    "1. single letters\n",
    "2. Fitment data included...tends to be very long, could limit column if it has >1000 characters\n",
    "    #RBP-LK215-40FS\tRBP\n",
    "     <BR/><BR/>\n",
    "    Çöno -> no\n",
    "    Repeating words\n",
    "    \n",
    "cleanStringColumn(CompiledJobber, \"JobberString\")\n",
    "(CompiledJobber.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\4. Temp Folder\\CompiledJobber.csv', index = None, header=True))\n",
    "\n",
    "CompiledJobber = CompiledJobber[~CompiledJobber.duplicated(subset=[\"sku\", \"brand\"], keep=\"first\") ]\n",
    "CompiledJobber = CompiledJobber[~CompiledJobber[\"sku\"].isna()]\n",
    "CompiledJobber = CompiledJobber[~CompiledJobber[\"JobberString\"].isna()]\n",
    "CompiledJobber = CompiledJobber[CompiledJobber[\"JobberString\"] != \"\"]\n",
    "\n",
    "(CompiledJobber.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Brand_Update_Location\\5_R_Brand_Reference_Files\\PSTprediction\\CompiledJobber.csv', index = None, header=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile all column names in MS and create refrence \n",
    "CompiledMainSheet = pd.DataFrame({\"sku\":[]})\n",
    "SubsetList = ['sku','type',\"product_name\", 'part_type_filter','series_parent']\n",
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "for b in range(len(AllBrandFolders)):\n",
    "    brand = AllBrandFolders[b]\n",
    "    print((len(AllBrandFolders)-b), brand)\n",
    " \n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+brand)\n",
    "    BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "    filecount = len(gl.glob(BrandMSpath))\n",
    "    if filecount == 1:\n",
    "        MSFile = gl.glob(BrandMSpath)[0]\n",
    "        MS = pd.read_csv(MSFile, encoding='mac_roman')\n",
    "        BrandMS_Col =  pd.DataFrame({\"col\":  list(map(str.strip, (MS.columns))) } )\n",
    "        MS.columns = BrandMS_Col[\"col\"]\n",
    "        MS = MS[MS[\"type\"] == \"simple\"][SubsetList]\n",
    "        MS[\"brand\"] = brand\n",
    "        \n",
    "        CompiledMainSheet = pd.concat([CompiledMainSheet,MS], axis=0, ignore_index=True, sort=False)\n",
    "        \n",
    "    \n",
    "CompiledMainSheetSS = (CompiledMainSheet[~(CompiledMainSheet[\"sku\"].isna()) &\n",
    "                                         ~(CompiledMainSheet[\"part_type_filter\"].isna()) & \n",
    "                                         ~(CompiledMainSheet[\"series_parent\"].isna()) &\n",
    "                                         (CompiledMainSheet[\"series_parent\"] != \"Discontinued\") & \n",
    "                                         (CompiledMainSheet[\"part_type_filter\"] != \"Discontinued\") \n",
    "                                          ]) [[\"sku\", \"brand\",\"product_name\", \"part_type_filter\", \"series_parent\"]]\n",
    "CompiledMainSheetSS = CompiledMainSheetSS[~CompiledMainSheetSS.duplicated(subset=[\"sku\", \"brand\"], keep=\"first\") ]\n",
    "\n",
    "CompiledMainSheetSS.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Brand_Update_Location\\5_R_Brand_Reference_Files\\PSTprediction\\CompiledMainSheetSS.csv', index = None, header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Analyze the MS title structure\n",
    "CompiledMainSheet=pd.read_csv(\"//192.168.2.32/Group/Data Team/Brand_Update_Location/5_R_Brand_Reference_Files/PSTprediction/CompiledMainSheetSS.csv\", encoding='utf-8')\n",
    "\n",
    "#Extract Clean Title\n",
    "CompiledMainSheet[\"CleanSku\"] = [s.replace(r'.', '#') for s in CompiledMainSheet[\"sku\"]]\n",
    "CompiledMainSheet[\"CleanSku\"] = [s.replace(r'#', '.*') for s in CompiledMainSheet[\"CleanSku\"]]\n",
    "CompiledMainSheet[\"CleanSku\"] = [s.replace(r'-', '.*') for s in CompiledMainSheet[\"CleanSku\"]]\n",
    "\n",
    "CompiledMainSheet[\"TitlePrefix\"] = \".*\" + CompiledMainSheet[\"CleanSku\"] + \" - \"\n",
    "\n",
    "cleantitlelist = []\n",
    "def cleantitlefun(p, titlelist = cleantitlelist):\n",
    "    title = re.sub(CompiledMainSheet.loc[p, \"TitlePrefix\"], \"\", CompiledMainSheet.loc[p, \"product_name\"])\n",
    "    titlelist = titlelist.append(title)\n",
    "    \n",
    "for p in range(len(CompiledMainSheet)): cleantitlefun(p)\n",
    "CompiledMainSheet[\"CleanTitle\"] = cleantitlelist\n",
    "CompiledMainSheet['CleanTitle'] = CompiledMainSheet['CleanTitle'].str.lower()\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'[^\\w]', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'[^a-zA-Z\\s]+', ' ')\n",
    "\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'\\s\\w\\s', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'\\s\\w\\w\\s' , ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'\\s\\w\\s', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'\\s\\w\\w\\s', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'^\\w\\s', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'^\\w\\w\\s', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'\\s\\w$' , ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'\\s\\w\\w$', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'^\\w$', ' ')\n",
    "CompiledMainSheet[\"CleanTitle\"] = CompiledMainSheet['CleanTitle'].str.replace(r'^\\w\\w$', ' ')\n",
    "\n",
    "CompiledMainSheet['CleanTitle'] = CompiledMainSheet['CleanTitle'].str.replace(r'   ', ' ')\n",
    "CompiledMainSheet['CleanTitle'] = CompiledMainSheet['CleanTitle'].str.replace(r'  ', ' ')\n",
    "CompiledMainSheet['CleanTitle'] = CompiledMainSheet['CleanTitle'].str.replace(r'   ', ' ')\n",
    "CompiledMainSheet['CleanTitle'] = CompiledMainSheet['CleanTitle'].str.replace(r'  ', ' ')\n",
    "CompiledMainSheet['CleanTitle'] = CompiledMainSheet['CleanTitle'].str.strip()\n",
    "\n",
    "\n",
    "titlelist = CompiledMainSheet[\"CleanTitle\"]+\"~~\"+CompiledMainSheet[\"part_type_filter\"]\n",
    "titlelist = pd.DataFrame({\"title\":titlelist})\n",
    "titlelist = titlelist[\"title\"].unique()\n",
    "titlelist = pd.DataFrame([t.split(r'~~') for t in titlelist])\n",
    "titlelist.columns = [\"title\", \"part_type_filter\"]\n",
    "titlelist = titlelist.groupby(\"part_type_filter\").agg(\"count\").sort_values(\"title\").reset_index()\n",
    "titlelist.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Brand_Update_Location\\5_R_Brand_Reference_Files\\PSTprediction\\titlelist.csv', index = None, header=True)\n",
    "\n",
    "PT = \"Fender Flares\"\n",
    "A = list(CompiledMainSheet[CompiledMainSheet[\"part_type_filter\"]==PT][\"CleanTitle\"].unique())\n",
    "\n",
    "\n",
    "#I need to come up with a strategy to analyze the titles...\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pd.DataFrame([pos_tag(nltk.word_tokenize(s)) for s in A])\n",
    "A = nltk.word_tokenize(CompiledMainSheet[\"CleanTitle\"][700188])\n",
    "pos_tag(A)\n",
    "\n",
    "CompiledMainSheet[CompiledMainSheet[\"sku\"] == \"#776678050\"] # #0124-4\n",
    "CompiledMainSheet[\"sku\"][700188]\n",
    "CompiledMainSheet.iloc[700188,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge jobber with Mainsheet\n",
    "#Load Files\n",
    "CompiledJobber=pd.read_csv(\"//192.168.2.32/Group/Data Team/Brand_Update_Location/5_R_Brand_Reference_Files/PSTprediction/CompiledJobber.csv\", encoding='utf-8')\n",
    "CompiledMainSheet=pd.read_csv(\"//192.168.2.32/Group/Data Team/Brand_Update_Location/5_R_Brand_Reference_Files/PSTprediction/CompiledMainSheetSS.csv\", encoding='utf-8')\n",
    "\n",
    "CompiledJobber[CompiledJobber[\"sku\"] == CompiledMainSheet[\"sku\"][700188]]\n",
    "\n",
    "\n",
    "TrainingSet = CompiledJobber.merge(CompiledMainSheet, on = [\"sku\", \"brand\"], how = \"left\")\n",
    "TrainingSet.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Brand_Update_Location\\5_R_Brand_Reference_Files\\PSTprediction\\TrainingSet.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare textual data for PT model\n",
    "TrainingSet=pd.read_csv(\"//192.168.2.32/Group/Data Team/Brand_Update_Location/5_R_Brand_Reference_Files/PSTprediction/TrainingSet.csv\", encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
